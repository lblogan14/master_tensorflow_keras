{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch6_rnn_tensorflow_keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/master_tensorflow_keras/blob/master/ch6_rnn_tensorflow_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "IlGB1uqA2S7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0931875f-3b50-42ae-c0ca-6d6064124331"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "print(\"NumPy:{}\".format(np.__version__))\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(123)\n",
        "print(\"TensorFlow:{}\".format(tf.__version__))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NumPy:1.14.6\n",
            "TensorFlow:1.12.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b1GSxx6Ljgmp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Recurrent Neural Network (RNN)** is a specialized neural network architecture for handling\n",
        "sequential data."
      ]
    },
    {
      "metadata": {
        "id": "-TFL0Sq6oNWB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Simple Recurrent Neural Network\n",
        "\n",
        "![](https://github.com/armando-fandango/Mastering-TensorFlow/blob/master/images/ch-06/06_01.png?raw=true)\n",
        "\n",
        "The neural Network $N$ takes input $x_t$ to produce output $y_t$. \n",
        "\n",
        "At the next time step $t+1$, it takes the input $y_t$ along with input $x_{t+1}$ to produce output $y_{t+1}$,\n",
        "$$y_t=\\phi(w^{(x)}\\times x_t+w^{(y)}\\times y_{t-1}+b)$$\n",
        "\n",
        "If we unroll the network at time step 5,\n",
        "![alt text](https://github.com/armando-fandango/Mastering-TensorFlow/blob/master/images/ch-06/06_03.png?raw=true)\n",
        "At every time step, the same learning function, $\\phi(\\cdot)$, and the same parameters, $w$ and $b$, are\n",
        "used."
      ]
    },
    {
      "metadata": {
        "id": "0zs7o5LVqgS3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also add hidden layers in RNN, then unroll it at time step 5 as well,\n",
        "![alt text](https://github.com/armando-fandango/Mastering-TensorFlow/blob/master/images/ch-06/06_04.png?raw=true)\n",
        "As you can see, the output $y$ is not always produced at every time step. Instead, an output $h$ is produced at\n",
        "every time step, and another activation function is applied to this output $h$ to produce the\n",
        "output $y$,\n",
        "$$h_t=\\phi(w^{(hx)}\\times x_t+w^{(hh)}\\times h_{t-1}+b^{(h)})$$\n",
        "$$y_t=\\phi(w^{(yh)}\\times h_t+b^{(y)})$$\n",
        "* $w^{(hx)}$ is the weight vector for $x$ inputs that are connected to the hidden layer\n",
        "* $w^{(hh)}$ is the weight vector for $h$ from the previous time step\n",
        "* $w^{(yh)}$ is the weight vector for layer connecting the hidden layer to the output layer\n",
        "* $h_t$ is usually a nonlinear function, such as $\\tanh$ or $ReLU$.\n",
        "\n",
        "In RNN, same parameters, $(w^{(hx)}, w^{(hh)}, w^{(yh)}, b^{(h)}, b^{(y)})$ are used at every time step."
      ]
    },
    {
      "metadata": {
        "id": "sBv7mIBAsNM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#RNN variants\n",
        "* **BIdirectional RNN (BRNN)** is used when the output depends on both the\n",
        "previous and future elements of a sequence. BRNN is implemented by stacking\n",
        "two RNNs, known as forward and backward Layer, and the output is the result\n",
        "of the hidden state of both the RNNs.\n",
        "* **Deep Bidirectional RNN (DBRNN)** extends the BRNN further by\n",
        "adding multiple layers. The BRNN has hidden layers or cells across the time\n",
        "dimensions. However, by stacking BRNN, we get the hierarchical presentation in\n",
        "DBRNN.\n",
        "* **Long Short-Term Memory (LSTM)** network extends the RNN by using an\n",
        "architecture that involves multiple nonlinear functions instead of one simple\n",
        "nonlinear function to compute the hidden state. The LSTM is composed of black\n",
        "boxes called cells that take the three inputs: the working memory, $h_{t-1}$ at time $t-1$, current input, $x_t$, and long-term memory $c_{t-1}$ at time $t-1$, and\n",
        "produce the two outputs: updated working memory, $h_t$, and long-term memory $c_t$. The cells use the functions known as gates, to make decisions about saving\n",
        "and erasing the content selectively from the memory.\n",
        "* **Gated Recurrent Unit (GRU)** network is a simplified variation of\n",
        "LSTM. It combines the function of the *forget* and the *input* gates in a\n",
        "simpler *update* gate. It also combines the *hidden* state and *cell* state into one single\n",
        "state. Hence, GRU is computationally less expensive as compared to LSTM.\n",
        "* **seq2seq** model combines the encoder-decoder architecture with RNN\n",
        "architectures. In seq2seq architecture, the model is trained on sequences of data,\n",
        "such as text data or time series data, and then the model is used to generate the\n",
        "output sequences. The seq2seq model consists of an encoder and a\n",
        "decoder model, both of them built with the RNN architecture. The seq2seq\n",
        "models can be stacked to build hierarchical multi-layer models."
      ]
    },
    {
      "metadata": {
        "id": "7PmN6E-3tx4m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##LSTM network\n",
        "When RNNs are trained over very long sequences of data, the gradients tend to become\n",
        "either very large or very small that they vanish to almost zero. **Long Short-Term Memory\n",
        "(LSTM)** networks address the vanishing/exploding gradient problem by adding gates for\n",
        "controlling the access to past information.\n",
        "\n",
        "In LSTM, a repeating module consisting of four main functions is used. This module that builds the LSTM is called the **cell**. The LSTM cell helps train the\n",
        "model more effectively when long sequences are passed, by selectively learning or erasing\n",
        "information. The functions composing the cell are also known as **gates** as they act as\n",
        "gatekeeper for the information that is passed in and out of the cell.\n",
        "\n",
        "LSTM model has two kinds of memory:\n",
        "* working memory, $h$ (hidden state)\n",
        "* long-term memory, $c$ (cell state)\n",
        "\n",
        "The cell state or long-term memory flows from cell to cell with only two linear interactions.\n",
        "The LSTM adds information to the long term memory, or removes information from the\n",
        "long-term memory, through gates, as shown below:\n",
        "![alt text](https://github.com/armando-fandango/Mastering-TensorFlow/blob/master/images/ch-06/06_05.png?raw=true)\n",
        "\n",
        "1. **Forget Gate** $f(\\cdot)$ (or remember gate): the $h_{t-1}$ and $x_t$ flows as input to $f(\\cdot)$ gate: $$f(\\cdot)=\\sigma(w^{(fx)}\\times x_t+w^{(fh)}\\times h_{t-1}+b^{(f)})$$\n",
        "The function of **forget gate** is to decide which information to forget and which\n",
        "information to remember. The *sigmoid* activation function is used here, so that an\n",
        "output of 1 represents that the information is carried over to the next step within\n",
        "the cell, and an output of 0 represents that the information is selectively\n",
        "discarded.\n",
        "2. **Input Gate** $i(\\cdot)$ (or save gate):  the $h_{t-1}$ and $x_t$ flows as input to $i(\\cdot)$ gate: \n",
        "$$i(\\cdot)=\\sigma(w^{(ix)}\\times x_t+w^{(ih)}\\times h_{t-1}+b^{(i)})$$\n",
        "The function of **input gate** is to decide whether to save or discard the input. The\n",
        "input function also allows the cell to learn which part of candidate memory to\n",
        "keep or discard.\n",
        "3. **Candidate Long-Term Memory**: Memory: The candidate long-term memory is computed\n",
        "from $h_{t-1}$ and $x_t$ using an activation function, which is mostly tanh,\n",
        "$$\\tilde c(\\cdot)=\\tanh(w^{(\\tilde c x)}\\times x_t+w^{(\\tilde c h)}\\times h_{t-1}+b^{(\\tilde c)})$$\n",
        "4. The preceding three calculations are combined to get the update long-term\n",
        "memory,\n",
        "$$c_t = c_{t-1}\\times f(\\cdot) + i(\\cdot)\\times\\tilde c(\\cdot)$$\n",
        "5. **Output Gate** $o(\\cdot)$ (or focus/attention gate): the $h_{t-1}$ and $x_t$ flows as input to $o(\\cdot)$ gate: \n",
        "$$o(\\cdot)=\\sigma(w^{(ox)}\\times x_t+w^{(oh)}\\times h_{t-1}+b^{(o)})$$\n",
        "The function of **output gate** is to decide how much information can be used to\n",
        "update the working memory.\n",
        "6. **Working memory** $h_t$ is updated from the long-term memory $c_t$ and the focus/attention vector $o(\\cdot)$:\n",
        "$$h_t=\\phi(c_t)\\times o(\\cdot)$$\n",
        "$\\phi(\\cdot)$ is activation function, usually $\\tanh$"
      ]
    },
    {
      "metadata": {
        "id": "yADyYEMwyT_B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##GRU network\n",
        "LSTM Network is computationally expensive, hence, researchers found an almost equally\n",
        "effective configuration of RNNs, known as **Gated Recurrent Unit (GRU)** architecture.\n",
        "\n",
        "In GRU, instead of a working and a long-term memory, only one kind of memory is used,\n",
        "indicated with $h$ (hidden state). The GRU cell adds information to this state memory or\n",
        "removes information from this state memory through reset and update gates.\n",
        "![alt text](https://github.com/armando-fandango/Mastering-TensorFlow/blob/master/images/ch-06/06_06.png?raw=true)\n",
        "\n",
        "1. **Update Gate** $u(\\cdot)$: the $h_{t-1}$ and $x_t$ flows as input to $u(\\cdot)$ gate: \n",
        "$$u(\\cdot)=\\sigma(w^{(ux)}\\times x_t+w^{(uh)}\\times h_{t-1}+b^{(u)})$$\n",
        "2. **Reset Gate** $r(\\cdot)$: the $h_{t-1}$ and $x_t$ flows as input to $r(\\cdot)$ gate: \n",
        "$$r(\\cdot)=\\sigma(w^{(rx)}\\times x_t+w^{(rh)}\\times h_{t-1}+b^{(r)})$$\n",
        "3. **Candidate State Memory**: The candidate long-term memory is computed from\n",
        "the output of the $r(\\cdot)$ gate, $h_{t-1}$, and $x_t$:\n",
        "$$\\tilde h(\\cdot)=\\tanh(w^{(\\tilde h x)}\\times x_t+w^{(\\tilde h h)}\\times h_{t-1}+b^{(\\tilde h)})$$\n",
        "4. The preceding three calculations are combined to get the updated state\n",
        "memory, $h_t$,\n",
        "$$h_t=(u_t\\cdot\\tilde h_t)+((1-u_t)\\cdot h_{t-1})$$"
      ]
    },
    {
      "metadata": {
        "id": "sPX8Zr3Hz0XB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#TensorFlow for RNN\n",
        "##TensorFlow RNN Cell Classes\n",
        "`tf.nn.rnn_cell` module provides\n",
        "\n",
        "Class | Description\n",
        "--- | ---\n",
        "BasicRNNCell | Provides simple RNN cell\n",
        "BasicLSTMCell | Provides simple LSTM RNN cell\n",
        "LSTMCell | Provides LSTM RNN cell\n",
        "GRUCell | Provides GRU RNN cell\n",
        "MultiRNNCell | Provides RNN cell made of multiple simple cells joined sequentially\n",
        "\n",
        "`tf.contrib.rnn` module provides\n",
        "\n",
        "Class | Description\n",
        "--- | ---\n",
        "LSTMBlockCell | Provides the block LSTM RNN cell\n",
        "LSTMBlockFusedCell | Provides the block fused LSTM RNN cell,\n",
        "GLSTMCell | Provides the group LSTM cell\n",
        "GridLSTMCell | Provides the grid LSTM RNN cel\n",
        "GRUBlockCell | Provides the block GRU RNN cell\n",
        "BidirectionalGridLSTMCell | Provides bidirectional grid LSTM with bi-direction only in frequency and not in time\n",
        "NASCell | Provides neural architecture search RNN cell\n",
        "UGRNNCell | Provides update gate RNN cell"
      ]
    },
    {
      "metadata": {
        "id": "8hX_06jn1BfK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##TensorFlow RNN Model Construction Classes\n",
        "TensorFlow provides classes to create RNN models from the RNN cell objects. The static\n",
        "RNN classes add unrolled cells for time steps at the compile time, while dynamic RNN\n",
        "classes add unrolled cells for time steps at the run time.\n",
        "* `tf.nn.static_rnn`\n",
        "* `tf.nn.static_state_saving_rnn`\n",
        "* `tf.nn.static_bidirectional_rnn`\n",
        "* `tf.nn.dynamic_rnn`\n",
        "*  `tf.nn.bidirectional_dynamic_rnn`\n",
        "*  `tf.nn.raw_rnn`\n",
        "*  `tf.contrib.rnn.stack_bidirectional_dynamic_rnn`"
      ]
    },
    {
      "metadata": {
        "id": "uumst6ww1XTP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##TensorFlow RNN Cell Wrapper Classes\n",
        "TensorFlow also provides classes that wrap other cell classes:\n",
        "* `tf.contrib.rnn.LSTMBlockWrapper`\n",
        "* `tf.contrib.rnn.DropoutWrapper`\n",
        "* `tf.contrib.rnn.EmbeddingWrapper`\n",
        "* `tf.contrib.rnn.InputProjectionWrapper`\n",
        "* `tf.contrib.rnn.OutputProjectionWrapper`\n",
        "* `tf.contrib.rnn.DeviceWrapper`\n",
        "* `tf.contrib.rnn.ResidualWrapper`"
      ]
    },
    {
      "metadata": {
        "id": "34Lo4jQ31ns4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Keras for RNN\n",
        "To build the RNN model, add layers from the `keras.layers.recurrent` module. This module contains `SimpleRNN`, `LSTM` and `GRU`\n",
        "\n",
        "###Stateful Models\n",
        "Keras recurrent layers also support RNN models that save state between the batches. You\n",
        "can create a stateful RNN, LSTM, or GRU model by passing stateful parameters as True.\n",
        "For stateful models, the batch size specified for the inputs has to be a fixed value. In stateful\n",
        "models, the hidden state learnt from training a batch is reused for the next batch. If you\n",
        "want to reset the memory at some point during training, it can be done with extra code by\n",
        "calling the `model.reset_states()` or `layer.reset_states()` functions."
      ]
    },
    {
      "metadata": {
        "id": "AoWDXgN92KTl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##RNN in Keras for MNIST data"
      ]
    },
    {
      "metadata": {
        "id": "lWVgdmQooXac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5a14511c-4a80-4e53-8d6e-ee1b145fb5da"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('mnist', one_hot=True)\n",
        "\n",
        "x_train = mnist.train.images\n",
        "x_test = mnist.test.images\n",
        "y_train = mnist.train.labels\n",
        "y_test = mnist.test.labels\n",
        "n_classes = 10"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting mnist/train-images-idx3-ubyte.gz\n",
            "Extracting mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aKYCqS-h2eZA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "x_train = x_train.reshape(-1, 28, 28)\n",
        "x_test = x_test.reshape(-1, 28, 28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2XkUoIbL2tX0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a1e69fe-6ef4-4f9b-99b9-df7eec26813c"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers.recurrent import SimpleRNN\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "iIM3dzgC2668",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "34mgy-et2-_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ffb25177-e6e9-483f-a8fe-d29151e951a9"
      },
      "cell_type": "code",
      "source": [
        "# build RNN\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=16, activation='relu', input_shape=(28, 28)))\n",
        "model.add(Dense(n_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_1 (SimpleRNN)     (None, 16)                720       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                170       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 890\n",
            "Trainable params: 890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TQ8JlD3J3Q9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "768429e1-868a-4430-b98c-c55dde79c644"
      },
      "cell_type": "code",
      "source": [
        "# compile and train\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.01),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=100, epochs=20)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "55000/55000 [==============================] - 4s 73us/step - loss: 1.2862 - acc: 0.5249\n",
            "Epoch 2/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 0.9538 - acc: 0.6639\n",
            "Epoch 3/20\n",
            "55000/55000 [==============================] - 4s 69us/step - loss: 0.8932 - acc: 0.6975\n",
            "Epoch 4/20\n",
            "55000/55000 [==============================] - 4s 69us/step - loss: 0.9650 - acc: 0.7096\n",
            "Epoch 5/20\n",
            "55000/55000 [==============================] - 4s 69us/step - loss: 10.9206 - acc: 0.2634\n",
            "Epoch 6/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 7/20\n",
            "55000/55000 [==============================] - 4s 69us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 8/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 9/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 10/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 11/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 12/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 13/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 14/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 15/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 16/20\n",
            "55000/55000 [==============================] - 4s 67us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 17/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 18/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 19/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n",
            "Epoch 20/20\n",
            "55000/55000 [==============================] - 4s 68us/step - loss: 14.5306 - acc: 0.0985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0cebc368d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "-5ndQDFU3h98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ef6f2e62-0554-438b-d1b9-ee1216e89f52"
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test)\n",
        "print('\\nTest loss:', score[0])\n",
        "print('Test Accuracy:', score[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 79us/step\n",
            "\n",
            "Test loss: 14.573981651306152\n",
            "Test Accuracy: 0.0958\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}