{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch18_debug_tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/master_tensorflow_keras/blob/master/ch18_debug_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Duf-j24uvfEf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c719f6e8-4344-4c3c-a880-a30a12d4d9eb"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(123)\n",
        "print(\"TensorFlow:{}\".format(tf.__version__))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow:1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4PMLHks-xDxU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "16f50a62-8515-4fcf-e47a-bea2ff8b4a18"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('mnist', one_hot=True)\n",
        "x_train = mnist.train.images\n",
        "x_test = mnist.test.images\n",
        "y_train = mnist.train.labels\n",
        "y_test = mnist.test.labels\n",
        "\n",
        "# parameters\n",
        "n_y = 10  # 0-9 digits\n",
        "n_x = 784  # total pixels"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting mnist/train-images-idx3-ubyte.gz\n",
            "Extracting mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R_T4sUN7xWzx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mlp(x, num_inputs, num_outputs,num_layers,num_neurons):\n",
        "    w=[]\n",
        "    b=[]\n",
        "    for i in range(num_layers):\n",
        "        # weights\n",
        "        w.append(tf.Variable(tf.random_normal( \n",
        "                              [num_inputs if i==0 else num_neurons[i-1], \n",
        "                               num_neurons[i]]), \n",
        "                             name=\"w_{0:04d}\".format(i) \n",
        "                            ) \n",
        "                ) \n",
        "        # biases\n",
        "        b.append(tf.Variable(tf.random_normal( \n",
        "                              [num_neurons[i]]), \n",
        "                             name=\"b_{0:04d}\".format(i) \n",
        "                            ) \n",
        "                )                   \n",
        "    w.append(tf.Variable(tf.random_normal(\n",
        "                          [num_neurons[num_layers-1] if num_layers > 0 else num_inputs,\n",
        "                           num_outputs]),name=\"w_out\"))\n",
        "    b.append(tf.Variable(tf.random_normal([num_outputs]),name=\"b_out\"))\n",
        "    \n",
        "    # x is input layer\n",
        "    layer = x\n",
        "    # add hidden layers\n",
        "    for i in range(num_layers):\n",
        "        layer = tf.nn.relu(tf.matmul(layer, w[i]) + b[i])\n",
        "    # add output layer\n",
        "    layer = tf.matmul(layer, w[num_layers]) + b[num_layers]\n",
        "    \n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ryx5HOsbwHE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Fetching tensor values with tf.Session.run()\n",
        "The values are\n",
        "returned as a NumPy array and can be printed or logged with Python statements. This is\n",
        "the simplest and easiest approach, with the biggest drawback being that the computation\n",
        "graph executes all the dependent paths, starting from the fetched tensor, and if those paths\n",
        "include the training operations, then it advances one step or one epoch."
      ]
    },
    {
      "metadata": {
        "id": "vrNYx9CqwZ7N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Printing tensor values with tf.Print()\n",
        "For debugging purposes you can use `tf.Print()`.\n",
        "\n",
        "    tf.Print(input_,\n",
        "             data,\n",
        "             message=None,\n",
        "             first_n=None,\n",
        "             summarize=None,\n",
        "             name=None)\n",
        "\n",
        "* `input_` is a tensor that gets returned from the function without anything being\n",
        "done to it\n",
        "* `data` is the list of tensors that get printed\n",
        "* `message` is a string that gets printed as a prefix to the printed output\n",
        "* `first_n` represents the number of steps to print the output; if this value is\n",
        "negative then the value is always printed whenever the path is executed\n",
        "* `summarize` represents the number of elements to print from the tensor; by\n",
        "default, only three elements are printed\n",
        "\n",
        "Let us modify the MNIST MLP model to add the print statement:"
      ]
    },
    {
      "metadata": {
        "id": "LBFFW4k8xvJ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KYHL7f_rx1hd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_layers = 2\n",
        "num_neurons = [16,32]\n",
        "learning_rate = 0.01\n",
        "n_epochs = 10\n",
        "batch_size = 100\n",
        "n_batches = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "# input images\n",
        "x_p = tf.placeholder(dtype=tf.float32, name=\"x_p\", shape=[None, n_x]) \n",
        "# target output\n",
        "y_p = tf.placeholder(dtype=tf.float32, name=\"y_p\", shape=[None, n_y]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "alXVz004x64i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "b180c5d4-8d7b-4788-b571-c87adc33bf78"
      },
      "cell_type": "code",
      "source": [
        "model = mlp(x=x_p, \n",
        "            num_inputs=n_x, \n",
        "            num_outputs=n_y, \n",
        "            num_layers=num_layers, \n",
        "            num_neurons=num_neurons)\n",
        "# tf.Print()\n",
        "model = tf.Print(input_=model,\n",
        "                 data=[tf.argmax(model,1)],\n",
        "                 message='y_hat=',\n",
        "                 summarize=10,\n",
        "                 first_n=5\n",
        "                )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-7e2bc71a2e99>:11: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
            "Instructions for updating:\n",
            "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
            "```python\n",
            "    sess = tf.Session()\n",
            "    with sess.as_default():\n",
            "        tensor = tf.range(10)\n",
            "        print_op = tf.print(tensor)\n",
            "        with tf.control_dependencies([print_op]):\n",
            "          out = tf.add(tensor, tensor)\n",
            "        sess.run(out)\n",
            "    ```\n",
            "Additionally, to use tf.print in python 2.7, users must make sure to import\n",
            "the following:\n",
            "\n",
            "  `from __future__ import print_function`\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aWAfNfaIyBju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "61b49481-6a5a-4b9d-da7f-d4fa164b60c5"
      },
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y_p))\n",
        "# optimizer function\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "optimizer = optimizer.minimize(loss)\n",
        "\n",
        "#predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
        "#accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-f6243f97184f>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uy0dP6LjyGEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2f5b0be0-5973-4a87-c79b-a8512af53fa0"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as tfs:\n",
        "        tfs.run(tf.global_variables_initializer())\n",
        "        for epoch in range(n_epochs):\n",
        "            epoch_loss = 0.0\n",
        "            for batch in range(n_batches):\n",
        "                X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
        "                feed_dict={x_p: X_batch, y_p: Y_batch}\n",
        "                _,batch_loss = tfs.run([optimizer,loss], \n",
        "                                       feed_dict = feed_dict\n",
        "                                      )\n",
        "                epoch_loss += batch_loss \n",
        "            average_loss = epoch_loss / n_batches\n",
        "            print(\"epoch: {0:04d}   loss = {1:0.6f}\".format(epoch,average_loss))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0000   loss = 6.536878\n",
            "epoch: 0001   loss = 2.225183\n",
            "epoch: 0002   loss = 1.925202\n",
            "epoch: 0003   loss = 1.798884\n",
            "epoch: 0004   loss = 1.710164\n",
            "epoch: 0005   loss = 1.639174\n",
            "epoch: 0006   loss = 1.579504\n",
            "epoch: 0007   loss = 1.529937\n",
            "epoch: 0008   loss = 1.487940\n",
            "epoch: 0009   loss = 1.450179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gYqUa9vOyNr-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Asserting on conditions with tf.Assert()\n",
        "The\n",
        "`tf.Assert()`` function takes a condition, and if the condition is false, it then prints the lists\n",
        "of given tensors and throws `tf.errors.InvalidArgumentError`.\n",
        "\n",
        "    tf.Assert(condition,\n",
        "              data,\n",
        "              summarize=None,\n",
        "              name=None)\n",
        "              \n",
        "To make sure that the `tf.Assert()` operation gets executed, we need\n",
        "to add it to the dependencies. For example, let us define an assertion to check that\n",
        "all the inputs are positive:\n",
        "\n",
        "`assert_op = tf.Assert(tf.reduce_all(tf.greater_equal(x,0)),[x])`\n",
        "The, add `assert_op` to the dependencies at the time of defining the model,\n",
        "\n",
        "    with tf.control_dependencies([assert_op]):\n",
        "        # x is input layer\n",
        "        layer = x\n",
        "        # add hidden layers\n",
        "        for i in range(num_layers):\n",
        "            layer = tf.nn.relu(tf.matmul(layer, w[i]) + b[i])\n",
        "        # add output layer\n",
        "        layer = tf.matmul(layer, w[num_layers]) + b[num_layers]"
      ]
    },
    {
      "metadata": {
        "id": "HOg5Fxjpy_m_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "55nUr6qIzBuE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_layers = 2\n",
        "num_neurons = [16,32]\n",
        "learning_rate = 0.01\n",
        "n_epochs = 10\n",
        "batch_size = 100\n",
        "n_batches = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "# input images\n",
        "x_p = tf.placeholder(dtype=tf.float32, name=\"x_p\", shape=[None, n_x]) \n",
        "# target output\n",
        "y_p = tf.placeholder(dtype=tf.float32, name=\"y_p\", shape=[None, n_y]) \n",
        "\n",
        "model = mlp(x=x_p, \n",
        "            num_inputs=n_x, \n",
        "            num_outputs=n_y, \n",
        "            num_layers=num_layers, \n",
        "            num_neurons=num_neurons)\n",
        "\n",
        "model = tf.Print(input_=model,\n",
        "                 data=[tf.argmax(model,1)],\n",
        "                 message='y_hat=',\n",
        "                 summarize=10,\n",
        "                 first_n=5\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ubXJKRFLzHeA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "11e3b2b7-7188-4724-a9fb-5d01b294babe"
      },
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y_p))\n",
        "# optimizer function\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "optimizer = optimizer.minimize(loss)\n",
        "\n",
        "#predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
        "#accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
        "\n",
        "with tf.Session() as tfs:\n",
        "        tfs.run(tf.global_variables_initializer())\n",
        "        for epoch in range(n_epochs):\n",
        "            epoch_loss = 0.0\n",
        "            for batch in range(n_batches):\n",
        "                X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
        "                if epoch > 5:\n",
        "                    X_batch = np.copy(X_batch)\n",
        "                    X_batch[0,0]=-2\n",
        "                feed_dict={x_p: X_batch, y_p: Y_batch}\n",
        "                _,batch_loss = tfs.run([optimizer,loss], \n",
        "                                       feed_dict = feed_dict\n",
        "                                      )\n",
        "                epoch_loss += batch_loss \n",
        "            average_loss = epoch_loss / n_batches\n",
        "            print(\"epoch: {0:04d}   loss = {1:0.6f}\".format(epoch,average_loss))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0000   loss = 6.260224\n",
            "epoch: 0001   loss = 2.239868\n",
            "epoch: 0002   loss = 2.086190\n",
            "epoch: 0003   loss = 1.993967\n",
            "epoch: 0004   loss = 1.927934\n",
            "epoch: 0005   loss = 1.876514\n",
            "epoch: 0006   loss = 1.835091\n",
            "epoch: 0007   loss = 1.799004\n",
            "epoch: 0008   loss = 1.767816\n",
            "epoch: 0009   loss = 1.734336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZeZjh3bgzQQ7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apart from the `tf.Assert()` function, which can take any valid conditional expression,\n",
        "TensorFlow provides the following assertion operations that check for specific conditions\n",
        "and have a simple syntax:\n",
        "* assert_equal\n",
        "* assert_greater\n",
        "* assert_greater_equal\n",
        "* assert_integer\n",
        "* assert_less\n",
        "* assert_less_equal\n",
        "*assert_negative\n",
        "* assert_none_equal\n",
        "* assert_non_negative\n",
        "* assert_non_positive\n",
        "* assert_positive\n",
        "* assert_proper_iterable\n",
        "* assert_rank\n",
        "* assert_rank_at_least\n",
        "* assert_rank_in\n",
        "* assert_same_float_dtype\n",
        "* assert_scalar\n",
        "* assert_type\n",
        "* assert_variables_initialized"
      ]
    },
    {
      "metadata": {
        "id": "GULTAywjzeVv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Debugging with the TensorFlow debugger (tfdbg)\n",
        "To use a debugger,\n",
        "1. Set the breakpoints in the code at locations where you want to break and inspect\n",
        "the variables\n",
        "2. Run the code in debug mode\n",
        "3. When the code breaks at a breakpoint, inspect it and then move on to next step"
      ]
    },
    {
      "metadata": {
        "id": "vKgoVuqlzpkT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GaLeDOROzvcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_layers = 2\n",
        "num_neurons = [16,32]\n",
        "learning_rate = 0.01\n",
        "n_epochs = 10\n",
        "batch_size = 100\n",
        "n_batches = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "# input images\n",
        "x_p = tf.placeholder(dtype=tf.float32, name=\"x_p\", shape=[None, n_x])\n",
        "# target output\n",
        "y_p = tf.placeholder(dtype=tf.float32, name=\"y_p\", shape=[None, n_y])\n",
        "\n",
        "model = mlp(x=x_p,\n",
        "            num_inputs=n_x,\n",
        "            num_outputs=n_y,\n",
        "            num_layers=num_layers,\n",
        "            num_neurons=num_neurons)\n",
        "\n",
        "model = tf.Print(input_=model,\n",
        "                 data=[tf.argmax(model,1)],\n",
        "                 message='y_hat=',\n",
        "                 summarize=10,\n",
        "                 first_n=5\n",
        "                )\n",
        "\n",
        "# loss function\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y_p))\n",
        "# optimizer function\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "optimizer = optimizer.minimize(loss)\n",
        "\n",
        "#predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
        "#accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
        "from tensorflow.python import debug as tfd\n",
        "\n",
        "with tfd.LocalCLIDebugWrapperSession(tf.Session()) as tfs:\n",
        "        tfs.run(tf.global_variables_initializer())\n",
        "        tfs.add_tensor_filter('has_inf_or_nan_filter', tfd.has_inf_or_nan)\n",
        "        for epoch in range(n_epochs):\n",
        "            epoch_loss = 0.0\n",
        "            for batch in range(n_batches):\n",
        "                X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
        "                if epoch > 0:\n",
        "                    X_batch = np.copy(X_batch)\n",
        "                    X_batch[0,0]=np.inf\n",
        "                feed_dict={x_p: X_batch, y_p: Y_batch}\n",
        "                _,batch_loss = tfs.run([optimizer,loss],\n",
        "                                       feed_dict = feed_dict\n",
        "                                      )\n",
        "                epoch_loss += batch_loss\n",
        "            average_loss = epoch_loss / n_batches\n",
        "            print(\"epoch: {0:04d}   loss = {1:0.6f}\".format(epoch,average_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}