{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch15_dist_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/master_tensorflow_keras/blob/master/ch15_dist_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xLt-qZlSZ7D6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Strategies for distributed execution\n",
        "For distributing the training of the single model across multiple devices or nodes, there are\n",
        "the following strategies:\n",
        "* **Model Parallel**: \\\\\n",
        "Divide the model into multiple subgraphs and place the separate\n",
        "graphs on different nodes or devices. The subgraphs perform their computation\n",
        "and exchange the variables as required.\n",
        "* **Data Parallel**: \\\\\n",
        "Divide the data into batches and run the same model on multiple\n",
        "nodes or devices, combining the parameters on a master node. Thus the worker\n",
        "nodes train the model on batches of data and send the parameter updates to the\n",
        "master node, also known as the parameter server.\n",
        "\n",
        "![](https://github.com/armando-fandango/Mastering-TensorFlow/blob/master/images/ch-15/15-01.png?raw=true)\n",
        "\n",
        "This diagram shows the data parallel approach where the model replicas read the partitions of data in batches and send the parameter updates to the parameter servers, and the parameter servers send the updated parameters back to the model replicas for the next batched computation of updates.\n",
        "\n",
        "In TensorFlow, there are two ways to implement replicating the model on multiple notes/devices under the data parallel strategy:\n",
        "* **In-Graph Replication**: \\\\\n",
        "There is a single client task that owns the\n",
        "model parameters and assigns the model calculations to multiple worker tasks.\n",
        "* **Between-Graph Replication**: \\\\\n",
        "Each client task connects to its\n",
        "own worker in order to assign the model calculation, but all workers update the\n",
        "same shared model. In this model, TensorFlow automatically assigns one worker\n",
        "to be the chief worker so that the model parameters are initialized only once by\n",
        "the chief worker.\n",
        "\n",
        "Within both these approaches, the parameters on the parameter servers can be updated in\n",
        "two different ways:\n",
        "* **Synchronous Update**: \\\\\n",
        "the parameter servers wait to\n",
        "receive the updates from all the workers before updating the gradients. The\n",
        "parameter server aggregates the updates, for example by calculating the mean of\n",
        "all the aggregates and applying them to the parameters. After the update, the\n",
        "parameters are sent to all the workers simultaneously. The disadvantage of this\n",
        "method is that one slow worker may slow down the updates for everyone.\n",
        "* **Asynchronous Update**: \\\\\n",
        "the workers send the\n",
        "updates to parameter server(s) as they are ready, and then the parameter server\n",
        "applies the updates as it receives them and sends them back. The disadvantage of\n",
        "this method is that by the time the worker calculates the parameters and sends\n",
        "the updates back, the parameters could have been updated several times by other\n",
        "workers. This problem can be alleviated by several methods such as lowering the\n",
        "batch size or lowering the learning rate."
      ]
    },
    {
      "metadata": {
        "id": "pgJ23j9Y6ok_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#TensorFlow clusters\n",
        "A *TensorFlow (TF) cluster* is one mechanism that implements the distributed strategies.\n",
        "\n",
        "At the logical level, a TF cluster runs one or more *jobs*, and each *job*\n",
        "consists of one or more *tasks*. Thus a *job* is just a logical grouping of the *tasks*. At the process\n",
        "level, each task runs as a TF server. At the machine level, each physical machine or node can\n",
        "run more than one task by running more than one server, one server per task. The *client*\n",
        "creates the graph on different servers and starts the execution of the graph on one server by\n",
        "calling the remote session.\n",
        "![alt text](https://github.com/armando-fandango/Mastering-TensorFlow/blob/master/images/ch-15/15-02.png?raw=true)\n",
        "\n",
        "This is how two clients connected to two jobs named `ml`. The two nodes are running three tasks each, and the job `w1` is spread across two nodes\n",
        "while the other jobs are contained within the nodes.\n",
        "\n",
        "To create and train a model in data parallel,\n",
        "1. Define the cluster specifications\n",
        "2. Create a server to host a task\n",
        "3. Define the variable nodes to be assigned to parameter server tasks\n",
        "4. Define the operation nodes to be replicated on all worker tasks\n",
        "5. Create a remote session\n",
        "6. Train the model in the remote session\n",
        "7. Use the model for prediction"
      ]
    },
    {
      "metadata": {
        "id": "XxdWbtS18Aam",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Define cluster spcification\n",
        "The cluster specification\n",
        "generally consists of two jobs: \n",
        "1. `ps` to create parameter server tasks\n",
        "2. `worker` to create worker tasks.\n",
        "\n",
        "    \n",
        "    clusterSpec = tf.train.ClusterSpec({\n",
        "        'ps': [\n",
        "            'master0.neurasights.com:2222', # /job:ps/task:0\n",
        "            'master1.neurasights.com:2222' # /job:ps/task:1\n",
        "              ]\n",
        "        'worker': [\n",
        "            'worker0.neurasights.com:2222', # /job:worker/task:0\n",
        "            'worker1.neurasights.com:2222', # /job:worker/task:1\n",
        "            'worker0.neurasights.com:2223', # /job:worker/task:2\n",
        "            'worker1.neurasights.com:2223' # /job:worker/task:3\n",
        "                  ]\n",
        "    })\n",
        "    \n",
        "This specification creates two jobs, with two tasks in job `ps` spread across two physical\n",
        "nodes and four tasks in job `worker` spread across two physical nodes.\n",
        "\n",
        "We can also create all tasks on a localhost, on different ports:\n",
        "\n",
        "    ps = [\n",
        "        'localhost:9001',    # /job:ps/task:0\n",
        "        ]\n",
        "    workers = [\n",
        "        'localhost:9002', # /job:worker/task:0\n",
        "        'localhost:9003', # /job:worker/task:1\n",
        "        'localhost:9004', # /job:worker/task:2\n",
        "        ]\n",
        "    clusterSpec = tf.train.ClusterSpec({'ps':ps, 'worker':workers})\n",
        "\n",
        "The tasks are identified with `/job:<job name>/task:<task index>`"
      ]
    },
    {
      "metadata": {
        "id": "9j9xsa3c-SiQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Create the server instances\n",
        "Since the cluster contains one server instance per task, on every physical node, start the\n",
        "servers by passing them the cluster specification, their own job name and task index. The\n",
        "servers use the cluster specification to figure out what other nodes are involved in the\n",
        "computation.\n",
        "\n",
        "    server = tf.train.Server(clusterSpec, job_name=\"ps\", task_index=0)\n",
        "    server = tf.train.Server(clusterSpec, job_name=\"worker\", task_index=0)\n",
        "    server = tf.train.Server(clusterSpec, job_name=\"worker\", task_index=1)\n",
        "    server = tf.train.Server(clusterSpec, job_name=\"worker\", task_index=2)\n",
        "    \n",
        "To run a single Python file on all the physical machines,\n",
        "\n",
        "    server = tf.train.Server(clusterSpec,\n",
        "                             job_name=FLAGS.task_index,\n",
        "                             config=config)\n",
        "                             \n",
        "The `job_name` and the `task_index` are taken from the parameters passed at\n",
        "the command line. The package, `tf.flags` is a fancy parser that gives you access to the\n",
        "command-line arguments.\n",
        "\n",
        "The Python file is executed as follows,\n",
        "\n",
        "    # the model should be run in each physical node\n",
        "    # using the appropriate arguments\n",
        "    $ python3 model.py --job_name='ps' --task_index=0\n",
        "$ python3 model.py --job_name='worker' --task_index=0\n",
        "    $ python3 model.py --job_name='worker' --task_index=1\n",
        "$ python3 model.py --job_name='worker' --task_index=2\n",
        "\n",
        "To ensure that our parameter server only uses CPU and our worker tasks use GPU, the configuration object is initialized,\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.allow_soft_placement = True\n",
        "    if FLAGS.job_name=='ps':\n",
        "        #print(config.device_count['GPU'])\n",
        "        config.device_count['GPU']=0\n",
        "        server = tf.train.Server(clusterSpec,\n",
        "                                 job_name=FLAGS.job_name,\n",
        "                                 task_index=FLAGS.task_index,\n",
        "                                 config=config\n",
        "                                 )\n",
        "        server.join()\n",
        "        sys.exit('0')\n",
        "    elif FLAGS.job_name=='worker':\n",
        "        config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "        server = tf.train.Server(clusterSpec,\n",
        "                                 job_name=FLAGS.job_name,\n",
        "                                 task_index=FLAGS.task_index,\n",
        "                                 config=config\n",
        "                                 )\n",
        "                                 \n",
        "The parameter server is made to wait with `server.join()` while the worker tasks execute\n",
        "the training of the model and exit."
      ]
    },
    {
      "metadata": {
        "id": "CZrEPHvvAVMW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Define the parameter and operations across servers and devices\n",
        "* use `tf.device()` function to place the parameters on the `ps` tasks and the computing nodes of the graphs on the `worker` tasks. \\\\\n",
        "Note that you can also place the graph nodes on specific devices by\n",
        "adding the device string to the task string as follows: \\\\\n",
        "`/job:<job name>/task:<task index>/device:<device type>:<device index>.`\n",
        "* use `tf.train.replica_device_setter()` to place the variables and operations\n",
        "  1. define the worker device to be the current worker: \\\\\n",
        "  `worker_device='/job:worker/task:{}'.format(FLAGS.task_index)`\n",
        "  2. define a device function using `replica_device_setter`\n",
        "  `device_func = tf.train.replica_device_setter(worker_device=worker_device,cluster=clusterSpec)`\n",
        "  3. create the graph inside the `tf.device(device_func)` block and train it. The creation and training of the graph is different for synchronous updates and asynchronous updates."
      ]
    },
    {
      "metadata": {
        "id": "tRkouDUBBwta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Define and train the graph for asynchronous updates\n",
        "In asynchronous updates all the\n",
        "worker tasks send the parameter updates when they are ready, and the parameter server\n",
        "updates the parameters and sends back the parameters. There is no synchronization or\n",
        "waiting or aggregation of parameter updates:\n",
        "![alt text](https://github.com/armando-fandango/Mastering-TensorFlow/blob/master/images/ch-15/15-04.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "mlq6-xI7CEFv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ESzTblCrCiHs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FLAGS = tf.flags.FLAGS\n",
        "\n",
        "tf.flags.DEFINE_string('job_name','ps','name of the job, default ps')\n",
        "tf.flags.DEFINE_integer('task_index',0,'index of the job, default 0')\n",
        "tf.flags.DEFINE_string('ps_hosts','localhost:9001','Comma-separated list of hostname:port pairs, default localhost:9001')\n",
        "tf.flags.DEFINE_string('worker_hosts','localhost:9002','Comma-separated list of hostname:port pairs, default localhost:9002')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLOP0KTnC7K6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For asynchronous updates, the graph is created and trained with the following steps:\n",
        "\n",
        "1. The definition of the graph is done within the `with` block. \\\\\n",
        "2. Create a `global step` variable using the inbuilt TensorFlow function.\n",
        "3. Define datasets, parameters,\n",
        "4. Define placehoders, weights, biases, logits, cross-entropy, loss op, train op, accuracy, etc...\n",
        "5. TensorFlow provides a supervisor class that helps in creating sessions for training\n",
        "and is very useful in a distributed training setting.\n",
        "6. Use the supervisor object to create a session and run the training under this\n",
        "session block"
      ]
    },
    {
      "metadata": {
        "id": "BmcjDEdQDnCS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(_):\n",
        "    mnist = input_data.read_data_sets('/home/armando/datasets/mnist', one_hot=True)\n",
        "\n",
        "    ps = FLAGS.ps_hosts.split(',')\n",
        "    workers = FLAGS.worker_hosts.split(',')\n",
        "\n",
        "    clusterSpec = tf.train.ClusterSpec({'ps': ps, 'worker': workers})\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.allow_soft_placement = True\n",
        "\n",
        "    #server = tf.train.Server(clusterSpec,\n",
        "    #                         job_name=FLAGS.job_name,\n",
        "    #                         task_index=FLAGS.task_index,\n",
        "    #                         config=config\n",
        "    #                         )\n",
        "\n",
        "    if FLAGS.job_name=='ps':\n",
        "        #print(config.device_count['GPU'])\n",
        "        config.device_count['GPU']=0\n",
        "        server = tf.train.Server(clusterSpec,\n",
        "                                 job_name=FLAGS.job_name,\n",
        "                                 task_index=FLAGS.task_index,\n",
        "                                 config=config\n",
        "                                 )\n",
        "        server.join()\n",
        "        sys.exit('0')\n",
        "    elif FLAGS.job_name=='worker':\n",
        "        config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "        server = tf.train.Server(clusterSpec,\n",
        "                                 job_name=FLAGS.job_name,\n",
        "                                 task_index=FLAGS.task_index,\n",
        "                                 config=config\n",
        "                                 )\n",
        "        is_chief = (FLAGS.task_index==0)\n",
        "\n",
        "        worker_device='/job:worker/task:{}'.format(FLAGS.task_index)\n",
        "        device_func = tf.train.replica_device_setter(worker_device=worker_device,\n",
        "                                                     cluster=clusterSpec\n",
        "                                                     )\n",
        "    # the default values are: ps_device='/job:ps',worker_device='/job:worker'\n",
        "        with tf.device(device_func):\n",
        "\n",
        "            global_step = tf.train.get_or_create_global_step()\n",
        "            #tf.Variable(0,name='global_step',trainable=False)\n",
        "            x_test = mnist.test.images\n",
        "            y_test = mnist.test.labels\n",
        "\n",
        "            # parameters\n",
        "            n_outputs = 10  # 0-9 digits\n",
        "            n_inputs = 784  # total pixels\n",
        "\n",
        "            learning_rate = 0.01\n",
        "            n_epochs = 50\n",
        "            batch_size = 100\n",
        "            n_batches = int(mnist.train.num_examples/batch_size)\n",
        "            n_epochs_print=10\n",
        "\n",
        "            # input images\n",
        "            x_p = tf.placeholder(dtype=tf.float32,\n",
        "                                 name='x_p',\n",
        "                                 shape=[None, n_inputs])\n",
        "            # target output\n",
        "            y_p = tf.placeholder(dtype=tf.float32,\n",
        "                                 name='y_p',\n",
        "                                 shape=[None, n_outputs])\n",
        "\n",
        "            w = tf.Variable(tf.random_normal([n_inputs, n_outputs],\n",
        "                                             name='w'\n",
        "                                             )\n",
        "                            )\n",
        "            b = tf.Variable(tf.random_normal([n_outputs],\n",
        "                                             name='b'\n",
        "                                             )\n",
        "                            )\n",
        "            logits = tf.matmul(x_p,w) + b\n",
        "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_p,\n",
        "                                                                    logits=logits\n",
        "                                                                    )\n",
        "            loss_op = tf.reduce_mean(cross_entropy)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "            train_op = optimizer.minimize(loss_op,global_step=global_step)\n",
        "            correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y_p, 1))\n",
        "            accuracy_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        sv = tf.train.Supervisor(is_chief=is_chief,\n",
        "                                 init_op = tf.global_variables_initializer(),\n",
        "                                 global_step=global_step)\n",
        "\n",
        "\n",
        "\n",
        "        with sv.prepare_or_wait_for_session(server.target) as mts:\n",
        "            lstep = 0\n",
        "\n",
        "            for epoch in range(n_epochs):\n",
        "                for batch in range(n_batches):\n",
        "                    x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "                    feed_dict={x_p:x_batch,y_p:y_batch}\n",
        "                    _,loss,gstep=mts.run([train_op,loss_op,global_step],\n",
        "                                         feed_dict=feed_dict)\n",
        "                    lstep +=1\n",
        "                if (epoch+1)%n_epochs_print==0:\n",
        "                    print('worker={},epoch={},global_step={}, local_step={}, loss = {}'.\n",
        "                          format(FLAGS.task_index,epoch,gstep,lstep,loss))\n",
        "            feed_dict={x_p:x_test,y_p:y_test}\n",
        "            accuracy = mts.run(accuracy_op, feed_dict=feed_dict)\n",
        "            print('worker={}, final accuracy = {}'.format(FLAGS.task_index,accuracy))\n",
        "    sv.stop()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZbohmT3zroF-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Define and train the graph for synchronous updates\n",
        "In synchronous updates, the tasks\n",
        "send their updates to the parameter servers, and ps tasks wait for all the updates to be\n",
        "received, aggregate them, and then update the parameters. The worker tasks wait for the\n",
        "updates before proceeding to the next iteration of computing parameter updates:\n",
        "\n",
        "![alt text](https://github.com/armando-fandango/Mastering-TensorFlow/blob/master/images/ch-15/15-05.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "gmQj5k2Kr7zS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For synchronous updates, the following modifications need to be made to the code:\n",
        "1. The optimizer needs to be wrapped in SyncReplicaOptimizer. Thus, after\n",
        "defining the optimizer, add the following code:\n",
        "\n",
        "\n",
        "    # SYNC: next line added for making it sync update\n",
        "    optimizer = tf.train.SyncReplicasOptimizer(optimizer,\n",
        "                                               replicas_to_aggregate=len(workers),\n",
        "                                               total_num_replicas=len(workers),\n",
        "                                               )\n",
        "2. This should be followed by adding the training operation as before:\n",
        "\n",
        "    \n",
        "    train_op = optimizer.minimize(loss_op,global_step=global_step)\n",
        "\n",
        "3. Next, add the initialization function definitions, specific to the synchronous\n",
        "update method:\n",
        "    \n",
        "    \n",
        "    if is_chief:\n",
        "        local_init_op = optimizer.chief_init_op()\n",
        "    else:\n",
        "        local_init_op = optimizer.local_step_init_op()\n",
        "    chief_queue_runner = optimizer.get_chief_queue_runner()\n",
        "    init_token_op = optimizer.get_init_tokens_op()\n",
        "    \n",
        "4. The supervisor object is also created differently with two additional initialization\n",
        "functions:\n",
        "\n",
        "    \n",
        "    # SYNC: sv is initialized differently for sync update\n",
        "    sv = tf.train.Supervisor(is_chief=is_chief,\n",
        "                             init_op = tf.global_variables_initializer(),\n",
        "                             local_init_op = local_init_op,\n",
        "                             ready_for_local_init_op = optimizer.ready_for_local_init_op,\n",
        "                             global_step=global_step)\n",
        "5. Finally, within the session block for training, we initialize the sync variables and\n",
        "start the queue runners if it is the chief worker task:\n",
        "\n",
        "\n",
        "    # SYNC: if block added to make it sync update\n",
        "    if is_chief:\n",
        "        mts.run(init_token_op)\n",
        "        sv.start_queue_runners(mts, [chief_queue_runner])\n",
        "\n",
        "The rest of the code remains the same as an asynchronous update."
      ]
    },
    {
      "metadata": {
        "id": "eSXAKr4vsx_D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FLAGS = tf.flags.FLAGS\n",
        "\n",
        "tf.flags.DEFINE_string('job_name','ps','name of the job, default ps')\n",
        "tf.flags.DEFINE_integer('task_index',0,'index of the job, default 0')\n",
        "\n",
        "def main(_):\n",
        "    mnist = input_data.read_data_sets('/home/armando/datasets/mnist', one_hot=True)\n",
        "\n",
        "    ps = [\n",
        "            'localhost:9001',  # /job:ps/task:0\n",
        "         ]\n",
        "    workers = [\n",
        "            'localhost:9002',  # /job:worker/task:0\n",
        "            'localhost:9003',  # /job:worker/task:1\n",
        "            'localhost:9004',  # /job:worker/task:2\n",
        "            ]\n",
        "    clusterSpec = tf.train.ClusterSpec({'ps': ps, 'worker': workers})\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.allow_soft_placement = True\n",
        "\n",
        "    if FLAGS.job_name=='ps':\n",
        "        #print(config.device_count['GPU'])\n",
        "        config.device_count['GPU']=0\n",
        "        server = tf.train.Server(clusterSpec,\n",
        "                                 job_name=FLAGS.job_name,\n",
        "                                 task_index=FLAGS.task_index,\n",
        "                                 config=config\n",
        "                                 )\n",
        "        server.join()\n",
        "        sys.exit('0')\n",
        "    elif FLAGS.job_name=='worker':\n",
        "        config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "        server = tf.train.Server(clusterSpec,\n",
        "                                 job_name=FLAGS.job_name,\n",
        "                                 task_index=FLAGS.task_index,\n",
        "                                 config=config\n",
        "                                 )\n",
        "        is_chief = (FLAGS.task_index==0)\n",
        "\n",
        "        worker_device='/job:worker/task:{}'.format(FLAGS.task_index)\n",
        "        device_func = tf.train.replica_device_setter(worker_device=worker_device,\n",
        "                                                     cluster=clusterSpec\n",
        "                                                     )\n",
        "    # the default values are: ps_device='/job:ps',worker_device='/job:worker'\n",
        "        with tf.device(device_func):\n",
        "\n",
        "            global_step = tf.train.get_or_create_global_step()\n",
        "            #tf.Variable(0,name='global_step',trainable=False)\n",
        "            x_test = mnist.test.images\n",
        "            y_test = mnist.test.labels\n",
        "\n",
        "            # parameters\n",
        "            n_outputs = 10  # 0-9 digits\n",
        "            n_inputs = 784  # total pixels\n",
        "\n",
        "            learning_rate = 0.01\n",
        "            n_epochs = 50\n",
        "            batch_size = 100\n",
        "            n_batches = int(mnist.train.num_examples/batch_size)\n",
        "            n_epochs_print=10\n",
        "\n",
        "            # input images\n",
        "            x_p = tf.placeholder(dtype=tf.float32,\n",
        "                                 name='x_p',\n",
        "                                 shape=[None, n_inputs])\n",
        "            # target output\n",
        "            y_p = tf.placeholder(dtype=tf.float32,\n",
        "                                 name='y_p',\n",
        "                                 shape=[None, n_outputs])\n",
        "\n",
        "            w = tf.Variable(tf.random_normal([n_inputs, n_outputs],\n",
        "                                             name='w'\n",
        "                                             )\n",
        "                            )\n",
        "\n",
        "            b = tf.Variable(tf.random_normal([n_outputs],\n",
        "                                             name='b'\n",
        "                                             )\n",
        "                            )\n",
        "\n",
        "            logits = tf.matmul(x_p,w) + b\n",
        "\n",
        "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_p,\n",
        "                                                                    logits=logits\n",
        "                                                                    )\n",
        "            loss_op = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "\n",
        "            # SYNC: next line added for making it sync update\n",
        "            optimizer = tf.train.SyncReplicasOptimizer(optimizer,\n",
        "                                                       replicas_to_aggregate=len(workers),\n",
        "                                                       total_num_replicas=len(workers),\n",
        "                                                       )\n",
        "\n",
        "            train_op = optimizer.minimize(loss_op,global_step=global_step)\n",
        "\n",
        "            # SYNC: next 6 lines added for making it sync update\n",
        "            if is_chief:\n",
        "                local_init_op = optimizer.chief_init_op()\n",
        "            else:\n",
        "                local_init_op = optimizer.local_step_init_op()\n",
        "            chief_queue_runner = optimizer.get_chief_queue_runner()\n",
        "            init_token_op = optimizer.get_init_tokens_op()\n",
        "\n",
        "            correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y_p, 1))\n",
        "            accuracy_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "        # SYNC: sv is initialized differently for sync update\n",
        "        sv = tf.train.Supervisor(is_chief=is_chief,\n",
        "                                 init_op = tf.global_variables_initializer(),\n",
        "                                 local_init_op = local_init_op,\n",
        "                                 ready_for_local_init_op = optimizer.ready_for_local_init_op,\n",
        "                                 global_step=global_step)\n",
        "\n",
        "\n",
        "        with sv.prepare_or_wait_for_session(server.target) as mts:\n",
        "\n",
        "            # SYNC: if block added to make it sync update\n",
        "            if is_chief:\n",
        "                mts.run(init_token_op)\n",
        "                sv.start_queue_runners(mts, [chief_queue_runner])\n",
        "\n",
        "            lstep = 0\n",
        "\n",
        "            for epoch in range(n_epochs):\n",
        "                for batch in range(n_batches):\n",
        "                    x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "                    feed_dict={x_p:x_batch,y_p:y_batch}\n",
        "                    _,loss,gstep=mts.run([train_op,loss_op,global_step], feed_dict=feed_dict)\n",
        "                    lstep +=1\n",
        "                if (epoch+1)%n_epochs_print==0:\n",
        "                    print('worker={},epoch={},global_step={}, local_step={}, loss = {}'.format(FLAGS.task_index,epoch,gstep,lstep,loss))\n",
        "            feed_dict={x_p:x_test,y_p:y_test}\n",
        "            accuracy = mts.run(accuracy_op, feed_dict=feed_dict)\n",
        "            print('worker={}, final accuracy = {}'.format(FLAGS.task_index,accuracy))\n",
        "#    sv.stop()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}