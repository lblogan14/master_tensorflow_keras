{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch9_cnn_tensorflow_keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/master_tensorflow_keras/blob/master/ch9_cnn_tensorflow_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "VL3cgX4UV-SM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Code Prerequisite"
      ]
    },
    {
      "metadata": {
        "id": "ufa8zGtlBpmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d069b01b-1120-400b-b2ea-db57c28a2116"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "print(\"NumPy:{}\".format(np.__version__))\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(123)\n",
        "print(\"TensorFlow:{}\".format(tf.__version__))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NumPy:1.14.6\n",
            "TensorFlow:1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fDVz7VaWWKRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "53ae8c78-af94-426c-8d99-186c0330178e"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My' 'Drive/Colab' 'Notebooks/Mastering_TensorFlow/data"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks/Mastering_TensorFlow/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MZEq6DRRWCWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "41db750d-e3f5-4926-9a9c-ab4053e4580d"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install imageio\n",
        "DATASETSLIB_HOME = '../datasetslib'\n",
        "import sys\n",
        "if not DATASETSLIB_HOME in sys.path:\n",
        "    sys.path.append(DATASETSLIB_HOME)\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "import datasetslib\n",
        "\n",
        "datasetslib.datasets_root = os.path.join(os.path.expanduser('~'),'datasets')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.14.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m0aKhhKKFV7h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Understanding convolution\n",
        "For example, let us assume the kernel matrix is a 2 x 2 matrix, and the input image is a 3 x 3\n",
        "matrix.\n",
        "\n",
        "![alt text](https://github.com/lblogan14/master_tensorflow_keras/blob/master/images/conv.JPG?raw=true)\n",
        "\n",
        "Then the feature map looks like the following:\n",
        "![alt text](https://github.com/lblogan14/master_tensorflow_keras/blob/master/images/fea_map.JPG?raw=true)\n",
        "\n",
        "The size of the feature maps is reduced by (kernel_size-1). The size of feature map is:\n",
        "$$size_{feature_map}=size_{features}-size_{kernel}+1$$\n",
        "\n",
        "**3D Tensor** - Convolution on 3D tensor is applied on each layer (channel) of the 3D tensor\n",
        "\n",
        "**stride** - how much the kernel slides across the tensor.\n",
        "$$size_{feature_map}=\\frac{size_{features}-size_{kernel}}{n_{strides}}+1$$\n",
        "\n",
        "**padding** - used on all sides of the input such that the size of features is increased by double of the padding size to avoid reducing the size of the feature map\n",
        "$$size_{feature_map}=\\frac{size_{features}+2*size_{padding}-size_{kernel}}{n_strides}+1$$"
      ]
    },
    {
      "metadata": {
        "id": "ELruibZkJd9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Convolution Operation in TensorFlow\n",
        "For example, the `tf.nn.conv2d()` operation,\n",
        "\n",
        "    tf.nn.conv2d(input,\n",
        "                 filter,\n",
        "                 strides,\n",
        "                 padding,\n",
        "                 use_cudnn_on_gpu=None,\n",
        "                 data_format=None,\n",
        "                 name=None\n",
        "                 )\n",
        "                               \n",
        "`input` and `filter` represent the data tensor of the shape `[batch_size, input_height,\n",
        "input_width, input_depth]` and kernel tensor of the shape `[filter_height,\n",
        "filter_width, input_depth, output_depth]`. The `output_depth` in the kernel tensor\n",
        "represents the number of kernels that should be applied to the input. The `strides` tensor\n",
        "represents the number of cells to slide in each dimension. The `padding` is VALID or SAME"
      ]
    },
    {
      "metadata": {
        "id": "yV3L8FNIKLFr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Understanding pooling\n",
        "**Pooling** refers to calculating the aggregate statistic over the regions of the convolved feature\n",
        "space.\n",
        "\n",
        "For example, a feature map of shape 3x3 and a pooling region of shape 2x2. Then the max-pooling is applied and shown in the following with stride of [1, 1],\n",
        "\n",
        "![alt text](https://github.com/lblogan14/master_tensorflow_keras/blob/master/images/maxpool.JPG?raw=true)\n",
        "\n",
        "Then, after max-pooling layer, we get\n",
        "\n",
        "![alt text](https://github.com/lblogan14/master_tensorflow_keras/blob/master/images/pool.JPG?raw=true)\n",
        "\n",
        "Generally, the pooling operation is applied with non-overlapping regions, thus the stride\n",
        "tensor and the region tensor are set to the same values.\n",
        "\n",
        "For TensorFlow, `max_pooling` can be called with the following initialziation,\n",
        "\n",
        "    max_poll(value,\n",
        "             ksize,\n",
        "             strides,\n",
        "             padding,\n",
        "             data_format='NHWC',\n",
        "             name=None\n",
        "             )\n",
        "             \n",
        "`value` represents the input tensor of the shape `[batch_size, input_height, input_width, input_depth]`. The pooling operation is performed on rectangular regions of shape `ksize`. These regions are offset by the shape `strides`"
      ]
    },
    {
      "metadata": {
        "id": "04E98DozAka2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#CNN architecture pattern - LeNet\n",
        "Build CNN model based on LeNet pattern by creating the layers in the following sequence:\n",
        "\n",
        "1. The input layer\n",
        "2. The convoluational layer 1 that produces a set of feature maps, with ReLU activation\n",
        "3. The pooling layer 1 that produces a set of statistically aggregated feature maps\n",
        "4. The convolutional layer 2 that produces a set of feature maps with ReLU activation\n",
        "5. The pooling layer 2 that produces a set of statistically aggregated feature maps\n",
        "6. The fully connected layer that flattens the feature maps with ReLU activation\n",
        "7. The output layer that produces the output by applying simple linear activation"
      ]
    },
    {
      "metadata": {
        "id": "fbWKRWVFBgcI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##LeNet for MNIST data"
      ]
    },
    {
      "metadata": {
        "id": "-AIfSE1yBt0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "2f696a20-e668-4387-f0ff-450cc71beffc"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('mnist', one_hot=True)\n",
        "\n",
        "x_train = mnist.train.images\n",
        "x_test = mnist.test.images\n",
        "y_train = mnist.train.labels\n",
        "y_test = mnist.test.labels"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-386decf3271c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting mnist/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iOqkUISOB_x0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#LeNet CNN for MNIST with TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "kk2QuqPKBj-b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c0GBCEvvCLT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "n_classes = 10\n",
        "n_width = 28\n",
        "n_height = 28\n",
        "n_depth = 1\n",
        "n_inputs = n_width * n_height * n_depth  # total pixels\n",
        "\n",
        "learning_rate = 0.001\n",
        "n_epochs = 10\n",
        "batch_size = 100\n",
        "n_batches = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "# input images in shape [n_samples, n_pixels]\n",
        "x = tf.placeholder(dtype=tf.float32, name='x', shape=[None, n_inputs])\n",
        "# output labels\n",
        "y = tf.placeholder(dtype=tf.float32, name='y', shape=[None, n_classes])\n",
        "\n",
        "# reshape input to [n_samples, n_width, n_height, n_depth]\n",
        "x_ = tf.reshape(x, shape=[-1, n_width, n_height, n_depth])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNQGssEVDMJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the first convolutional layer with 32 kernels of shape 4 x 4, thus producing\n",
        "32 feature maps.\n",
        "\n",
        "*   define the weights and biases first\n"
      ]
    },
    {
      "metadata": {
        "id": "pahA5wraDJHp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layer1_w = tf.Variable(tf.random_normal(shape=[4,4,n_depth,32], \n",
        "                                        stddev=0.1),\n",
        "                      name='l1_w')\n",
        "layer1_b = tf.Variable(tf.random_normal([32]),\n",
        "                      name='l1_b')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FtqJhmf9EI8f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   define the convolutional layer with `tf.nn.conv2d`. Note that `stride` defines the elements by which the kernel tensor should slide in each dimension. The dimension order is determined by `data_format`, which could be either `NHWC` or `NCHW`. Generally, the first and last element in `stride` is set to `1`. `padding` could be `SAME` or `VALID`. Remember to add `relu` activation after calling `tf.nn.conv2d`"
      ]
    },
    {
      "metadata": {
        "id": "Eb-jz2EgExNe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layer1_conv = tf.nn.relu(tf.nn.conv2d(x_,\n",
        "                                      layer1_w,\n",
        "                                      strides=[1,1,1,1],\n",
        "                                      padding='SAME'\n",
        "                                     ) + \n",
        "                         layer1_b\n",
        "                        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5BWkN33tFXKS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the first pooling layer with the `tf.nn.max_pool()` function. `ksize` represents the pooling operation using 2 x 2 x 1 regions, and `strides` represents to slide the regions by 2 x 2 x 1 pixels. Thus the regions do not overlap with each other."
      ]
    },
    {
      "metadata": {
        "id": "xTtbFVMbHam5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layer1_pool = tf.nn.max_pool(layer1_conv,\n",
        "                             ksize=[1,2,2,1],\n",
        "                             strides=[1,2,2,1],\n",
        "                             padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "erc4kYHqIXLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first convolution layer produces 32 feature maps of size 28 x 28 x 1, which are\n",
        "then pooled into data of shape 32 x 14 x 14 x 1.\n",
        "\n",
        "Now move on to the second convolutional layer,"
      ]
    },
    {
      "metadata": {
        "id": "956MUiAJJTTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layer2_w = tf.Variable(tf.random_normal(shape=[4,4,32,64],\n",
        "                                        stddev=0.1),\n",
        "                       name='l2_w')\n",
        "layer2_b = tf.Variable(tf.random_normal([64]),\n",
        "                       name='l2_b')\n",
        "\n",
        "layer2_conv = tf.nn.relu(tf.nn.conv2d(layer1_pool,\n",
        "                                      layer2_w,\n",
        "                                      strides=[1,1,1,1],\n",
        "                                      padding='SAME'\n",
        "                                     ) + \n",
        "                         layer2_b\n",
        "                        )\n",
        "layer2_pool = tf.nn.max_pool(layer2_conv,\n",
        "                             ksize=[1,2,2,1],\n",
        "                             strides=[1,2,2,1],\n",
        "                             padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZBzIxAO6LRxu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The second convolutional layer that takes this data as input and produces\n",
        "64 feature maps. The output of the second convolution layer is of shape 64 x 14 x 14 x 1, which then\n",
        "gets pooled into an output of shape 64 x 7 x 7 x 1."
      ]
    },
    {
      "metadata": {
        "id": "tPBMPh5xLe78",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reshape this output before feeding into the fully connected layer of 1024 neurons\n",
        "to produce a flattened output of size 1024:"
      ]
    },
    {
      "metadata": {
        "id": "JaJjKv5dLjgf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layer3_w = tf.Variable(tf.random_normal(shape=[64*7*7*1, 1024],\n",
        "                                        stddev=0.1),\n",
        "                       name='l3_w')\n",
        "layer3_b = tf.Variable(tf.random_normal([1024]),\n",
        "                       name='l3_b')\n",
        "\n",
        "layer3_fc = tf.nn.relu(tf.matmul(tf.reshape(layer2_pool, \n",
        "                                            [-1, 64*7*7*1]),\n",
        "                                 layer3_w) + \n",
        "                       layer3_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0ch6uctQSRK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The output of the fully connected layer is fed into a linear output layer with 10 outputs. The softmax function is not applied here because it will be automatically applied in the loss function,"
      ]
    },
    {
      "metadata": {
        "id": "OMra48joQcX3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layer4_w = tf.Variable(tf.random_normal(shape=[1024, n_classes],\n",
        "                                        stddev=0.1),\n",
        "                       name='l4_w')\n",
        "layer4_b = tf.Variable(tf.random_normal([n_classes]),\n",
        "                       name='l4_b')\n",
        "\n",
        "layer4_out = tf.matmul(layer3_fc, layer4_w) + layer4_b\n",
        "\n",
        "model = layer4_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vlPPBFCPQ-oF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "6589e9be-1bf6-43ad-915b-1efa251fe680"
      },
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y)\n",
        "loss = tf.reduce_mean(entropy)\n",
        "\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-1561934a5335>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2zb4lKPfRZtT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now everything is all set. We can start to train the CNN model,"
      ]
    },
    {
      "metadata": {
        "id": "LeKPwUTiRfyd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "91c8d10b-0c80-400e-b900-db590efba48e"
      },
      "cell_type": "code",
      "source": [
        "# train and evaluate\n",
        "with tf.Session() as tfs:\n",
        "  tf.global_variables_initializer().run()\n",
        "  for epoch in range(n_epochs):\n",
        "    total_loss = 0.0\n",
        "    for batch in range(n_batches):\n",
        "      x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "      feed_dict = {x:x_batch, y:y_batch}\n",
        "      batch_loss, _ = tfs.run([loss, optimizer], feed_dict=feed_dict)\n",
        "      total_loss += batch_loss\n",
        "    average_loss = total_loss / n_batches\n",
        "    print('Epoch: {0:04d}    loss = {1:0.6f}'.format(epoch, average_loss))\n",
        "  print('Model Trained.')\n",
        "  \n",
        "  predictions_check = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
        "  feed_dict = {x:x_test, y:y_test}\n",
        "  print('Accuracy: ', accuracy.eval(feed_dict=feed_dict))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0000    loss = 2.197315\n",
            "Epoch: 0001    loss = 0.084367\n",
            "Epoch: 0002    loss = 0.058146\n",
            "Epoch: 0003    loss = 0.046801\n",
            "Epoch: 0004    loss = 0.035882\n",
            "Epoch: 0005    loss = 0.031906\n",
            "Epoch: 0006    loss = 0.023579\n",
            "Epoch: 0007    loss = 0.023606\n",
            "Epoch: 0008    loss = 0.019154\n",
            "Epoch: 0009    loss = 0.015893\n",
            "Model Trained.\n",
            "Accuracy:  0.9876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HxN3DRwRS-PH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#LeNet CNN for MNIST with Keras"
      ]
    },
    {
      "metadata": {
        "id": "ptFebITzTCps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef781a01-cc4f-469d-d564-cc50b1caaf73"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Reshape\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "S4ugIWyeTR90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h1zDbkubTY_f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the number of filters for each layer"
      ]
    },
    {
      "metadata": {
        "id": "dsq7okq4TVZs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_filters = [32, 64]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vvMPWAGdTfpL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 10\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S4tkDJwPToAm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the sequential model and add the layer to reshape the input data to shape\n",
        "`(n_width,n_height,n_depth)`:"
      ]
    },
    {
      "metadata": {
        "id": "fi6AA6LWTlw8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Reshape(target_shape=(n_width, n_height, n_depth),\n",
        "                  input_shape=(n_inputs,)\n",
        "                 )\n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jG-AuHH3UAh0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Add the first convolutional layer with 4 x 4 kernel filter, `SAME` padding and `relu`\n",
        "activation:"
      ]
    },
    {
      "metadata": {
        "id": "gnc6pGZMUDPL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(Conv2D(filters=n_filters[0],\n",
        "                 kernel_size=4,\n",
        "                 padding='SAME',\n",
        "                 activation='relu'\n",
        "                )\n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8HG7DCuJUJZo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Add the pooling layer with region size of 2 x 2 and stride of 2 x 2:"
      ]
    },
    {
      "metadata": {
        "id": "3dX1ck2IUXw7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(MaxPooling2D(pool_size=(2,2),\n",
        "                       strides=(2,2)\n",
        "                      )\n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kf7zi9hNUefM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The second convolutional layer,"
      ]
    },
    {
      "metadata": {
        "id": "aeCMz91dUg-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(Conv2D(filters=n_filters[1],\n",
        "                 kernel_size=4,\n",
        "                 padding='SAME',\n",
        "                 activation='relu',\n",
        "                )\n",
        "         )\n",
        "model.add(MaxPooling2D(pool_size=(2,2),\n",
        "                       strides=(2,2)\n",
        "                      )\n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kN_b8ZrZU2Rq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Add a layer to flatten the output of the second pooling layer and a fully\n",
        "connected layer of 1024 neurons to handle the flattened output:"
      ]
    },
    {
      "metadata": {
        "id": "c6x9j03DU3be",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units=1024, activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tAuzp4cYVCMI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Add the final output layer with the `softmax` activation:"
      ]
    },
    {
      "metadata": {
        "id": "1CmHrmmIVD9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "7b2a8a66-d8db-4667-c358-e3f3bcd3d61e"
      },
      "cell_type": "code",
      "source": [
        "model.add(Dense(units=n_classes, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        544       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        32832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              3212288   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 3,255,914\n",
            "Trainable params: 3,255,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m-IXSnYWVO8r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train and evaluate,"
      ]
    },
    {
      "metadata": {
        "id": "XRpZcGFPVRPZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "d0d92b84-955a-4839-8c52-0d89fec08081"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(lr=learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epochs)\n",
        "\n",
        "score=model.evaluate(x_test, y_test)\n",
        "print('\\nTest loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "55000/55000 [==============================] - 8s 140us/step - loss: 0.9424 - acc: 0.7544\n",
            "Epoch 2/10\n",
            "55000/55000 [==============================] - 7s 133us/step - loss: 0.2490 - acc: 0.9253\n",
            "Epoch 3/10\n",
            "55000/55000 [==============================] - 7s 134us/step - loss: 0.1760 - acc: 0.9475\n",
            "Epoch 4/10\n",
            "55000/55000 [==============================] - 7s 134us/step - loss: 0.1368 - acc: 0.9596\n",
            "Epoch 5/10\n",
            "55000/55000 [==============================] - 7s 134us/step - loss: 0.1125 - acc: 0.9661\n",
            "Epoch 6/10\n",
            "55000/55000 [==============================] - 7s 133us/step - loss: 0.0961 - acc: 0.9711\n",
            "Epoch 7/10\n",
            "55000/55000 [==============================] - 7s 134us/step - loss: 0.0842 - acc: 0.9751\n",
            "Epoch 8/10\n",
            "55000/55000 [==============================] - 7s 133us/step - loss: 0.0751 - acc: 0.9778\n",
            "Epoch 9/10\n",
            "55000/55000 [==============================] - 7s 134us/step - loss: 0.0686 - acc: 0.9801\n",
            "Epoch 10/10\n",
            "55000/55000 [==============================] - 7s 134us/step - loss: 0.0623 - acc: 0.9809\n",
            "10000/10000 [==============================] - 1s 97us/step\n",
            "\n",
            "Test loss: 0.061484189332742245\n",
            "Test accuracy: 0.9796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tZkuDE9nWniF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#LeNet for CIFAR10 Data\n",
        "The CIFAR-10 dataset consists of 60,000 RGB color images of the shape 32x32 pixels. The\n",
        "images are equally divided into 10 different categories or classes: airplane, automobile, bird,\n",
        "cat, deer, dog, frog, horse, ship, and truck. CIFAR-10 and CIFAR-100 are subsets of a large\n",
        "image dataset comprising of 80 million images.\n",
        "## Get the CIFAR10 Data"
      ]
    },
    {
      "metadata": {
        "id": "jNcYoQMq3AfG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "37ad03e0-043b-4bbc-fcd7-b0157830de02"
      },
      "cell_type": "code",
      "source": [
        "from datasetslib.cifar import cifar10\n",
        "from datasetslib import imutil\n",
        "dataset = cifar10()\n",
        "dataset.x_layout=imutil.LAYOUT_NHWC\n",
        "dataset.load_data()\n",
        "dataset.scaleX()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Downloaded : /root/datasets/cifar10/cifar-10-python.tar.gz ( 170498071 bytes)\n",
            "Extracting  /root/datasets/cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EBMv0AXU3TLX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data is loaded such that the images are in the `'NHWC'` format, that makes the data\n",
        "variable of shape `(number_of_samples, image_height, image_width,\n",
        "image_channels)`. We refer to image channels as image depth. Each pixel in the images is a\n",
        "number from 0 to 255. The dataset is scaled using MinMax scaling to normalize the images\n",
        "by dividing all pixel values with 255.\n",
        "\n",
        "The loaded and pre-processed data becomes available in the dataset object variables as\n",
        "dataset.`X_train, dataset.Y_train, dataset.X_test, and dataset.Y_test`."
      ]
    },
    {
      "metadata": {
        "id": "nmtGSODw3798",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##ConvNets for CIFAR10 with TensorFlow\n",
        "Keep the layers, filters, and their sizes the same as in the MNIST examples earlier, with\n",
        "one new addition of regularization layer. \n",
        "\n",
        "Since this data set is complex as compared to the\n",
        "MNIST, we add additional dropout layers for the purpose of regularization:\n",
        "\n",
        "    tf.nn.dropout(layer, keep_prob)\n",
        "    \n",
        "The placeholder `keep_prob` is set to 1 during prediction and evaluation.\n",
        "That way we can reuse the same model for training as well as prediction\n",
        "and evaluation."
      ]
    },
    {
      "metadata": {
        "id": "02hIqbBs4dg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YTGlh0sz4f5r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "n_filters = [32, 64]\n",
        "learning_rate = 0.001\n",
        "\n",
        "# input images of shape = (n_samples, n_width, n_height, n_depth)\n",
        "x = tf.placeholder(dtype=tf.float32, name='x',\n",
        "                   shape=[None, dataset.width, dataset.height, dataset.depth])\n",
        "# target output\n",
        "y = tf.placeholder(dtype=tf.float32, name='y',\n",
        "                   shape=[None, dataset.n_classes])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# conv layer 1\n",
        "layer1_w = tf.Variable(tf.random_normal(shape=[4,4,dataset.depth,n_filters[0]],\n",
        "                                        stddev=0.01),\n",
        "                       name='l1_w'\n",
        "                      )\n",
        "layer1_b = tf.Variable(tf.random_normal([n_filters[0]]),\n",
        "                       name='l1_b'\n",
        "                      )\n",
        "layer1_conv = tf.nn.relu(tf.nn.conv2d(x,\n",
        "                                      layer1_w,\n",
        "                                      strides=[1,1,1,1],\n",
        "                                      padding='SAME'\n",
        "                                     ) +\n",
        "                         layer1_b\n",
        "                        )\n",
        "# pooling layer 1\n",
        "layer1_pool = tf.nn.max_pool(layer1_conv,\n",
        "                             ksize=[1,2,2,1],\n",
        "                             strides=[1,2,2,1],\n",
        "                             padding='SAME')\n",
        "layer1_pool = tf.nn.dropout(layer1_pool, keep_prob)\n",
        "\n",
        "# conv layer 2\n",
        "layer2_w = tf.Variable(tf.random_normal(shape=[4,4,n_filters[0],n_filters[1]],\n",
        "                                        stddev=0.01),\n",
        "                       name='l2_w'\n",
        "                      )\n",
        "layer2_b = tf.Variable(tf.random_normal([n_filters[1]]),\n",
        "                       name='l2_b'\n",
        "                      )\n",
        "layer2_conv = tf.nn.relu(tf.nn.conv2d(layer1_pool,\n",
        "                                      layer2_w,\n",
        "                                      strides=[1,1,1,1],\n",
        "                                      padding='SAME'\n",
        "                                     ) +\n",
        "                         layer2_b\n",
        "                        )\n",
        "# pooling layer 2\n",
        "layer2_pool = tf.nn.max_pool(layer2_conv,\n",
        "                             ksize=[1,2,2,1],\n",
        "                             strides=[1,2,2,1],\n",
        "                             padding='SAME')\n",
        "layer2_pool = tf.nn.dropout(layer2_pool, keep_prob)\n",
        "\n",
        "# fully connected layer\n",
        "layer3_w = tf.Variable(tf.random_normal(shape=[8*8*64, 1024],\n",
        "                                        stddev=0.01),\n",
        "                       name='l3_w'\n",
        "                      )\n",
        "layer3_b = tf.Variable(tf.random_normal([1024]),\n",
        "                       name='l3_b')\n",
        "layer3_fc = tf.nn.relu(tf.matmul(tf.reshape(layer2_pool, [-1, 8*8*64]),\n",
        "                                 layer3_w) + \n",
        "                       layer3_b\n",
        "                      )\n",
        "layer3_fc = tf.nn.dropout(layer3_fc, keep_prob)\n",
        "\n",
        "# output layer\n",
        "layer4_w = tf.Variable(tf.random_normal(shape=[1024, dataset.n_classes],\n",
        "                                        stddev=0.01),\n",
        "                       name='l4_w'\n",
        "                      )\n",
        "layer4_b = tf.Variable(tf.random_normal([dataset.n_classes]),\n",
        "                       name='l4_b'\n",
        "                      )\n",
        "layer4_out = tf.matmul(layer3_fc, layer4_w) + layer4_b\n",
        "\n",
        "model = layer4_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BYRSNkna9SiY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y)\n",
        "loss = tf.reduce_mean(entropy)\n",
        "# optimizer\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Wgurl97-uZu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "dataset.y_onehot = True\n",
        "dataset.batch_size = 128\n",
        "dataset.batch_shuffle = True\n",
        "n_batches = int(dataset.n_train/dataset.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1CGRXD2n-77d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a866a0cd-3772-417b-de39-fbfd56c78ff2"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as tfs:\n",
        "  tf.global_variables_initializer().run()\n",
        "  for epoch in range(n_epochs):\n",
        "    total_loss = 0.0\n",
        "    for batch in range(n_batches):\n",
        "      x_batch, y_batch = dataset.next_batch()\n",
        "      feed_dict = {x:x_batch, y:y_batch, keep_prob:1.0}\n",
        "      batch_loss, _ = tfs.run([loss, optimizer], feed_dict=feed_dict)\n",
        "      total_loss +=batch_loss\n",
        "    average_loss = total_loss / n_batches\n",
        "    print(\"Epoch: {0:04d}   loss = {1:0.6f}\".format(epoch,average_loss))\n",
        "  print(\"Model Trained.\")\n",
        "\n",
        "  predictions_check = tf.equal(tf.argmax(model,1),tf.argmax(y,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
        "  feed_dict = {x:dataset.X_test, y:dataset.Y_test, keep_prob: 1.0}\n",
        "  print(\"Accuracy:\", accuracy.eval(feed_dict=feed_dict))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0000   loss = 2.190397\n",
            "Epoch: 0001   loss = 1.744629\n",
            "Epoch: 0002   loss = 1.515042\n",
            "Epoch: 0003   loss = 1.365483\n",
            "Epoch: 0004   loss = 1.245399\n",
            "Epoch: 0005   loss = 1.142796\n",
            "Epoch: 0006   loss = 1.043091\n",
            "Epoch: 0007   loss = 0.940676\n",
            "Epoch: 0008   loss = 0.840083\n",
            "Epoch: 0009   loss = 0.745871\n",
            "Epoch: 0010   loss = 0.649716\n",
            "Epoch: 0011   loss = 0.557851\n",
            "Epoch: 0012   loss = 0.478131\n",
            "Epoch: 0013   loss = 0.396767\n",
            "Epoch: 0014   loss = 0.330925\n",
            "Epoch: 0015   loss = 0.273449\n",
            "Epoch: 0016   loss = 0.225828\n",
            "Epoch: 0017   loss = 0.187990\n",
            "Epoch: 0018   loss = 0.162484\n",
            "Epoch: 0019   loss = 0.140358\n",
            "Model Trained.\n",
            "Accuracy: 0.6315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8NbCqKF5_mCt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To increase the accuracy, modify the CNN architecture"
      ]
    },
    {
      "metadata": {
        "id": "iCghgWED_yrf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##ConvNets for CIFAR10 with Keras\n",
        "The dropout layer in Keras is\n",
        "\n",
        "    model.add(Dropout(0.2))"
      ]
    },
    {
      "metadata": {
        "id": "ipAlG2Yp_sEd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D, Dense, Flatten, Reshape, Dropout\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ac-YhZL_Abzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QiM9vDy-Af0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_filters = [32, 64]\n",
        "learning_rate = 0.001\n",
        "n_epochs = 20\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQS3NSfOAtU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "ee7cf891-526f-4d0c-ab46-54df99c062ed"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# conv layer 1\n",
        "model.add(Conv2D(filters=n_filters[0],\n",
        "                 kernel_size=4,\n",
        "                 padding='SAME',\n",
        "                 activation='relu',\n",
        "                 input_shape=(dataset.height, dataset.width, dataset.depth)\n",
        "                )\n",
        "         )\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# conv layer 2\n",
        "model.add(Conv2D(filters=n_filters[1],\n",
        "                 kernel_size=4,\n",
        "                 padding='SAME',\n",
        "                 activation='relu'\n",
        "                )\n",
        "         )\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units=1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=dataset.n_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        1568      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        32832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              4195328   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 4,239,978\n",
            "Trainable params: 4,239,978\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mE3kGfdOFDoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "a10f8429-f57f-4d5b-f928-30c1cc9bb7aa"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(dataset.X_train, dataset.Y_train, batch_size=dataset.batch_size, epochs=n_epochs)\n",
        "score = model.evaluate(dataset.X_test, dataset.Y_test)\n",
        "print('\\nTest loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 9s 182us/step - loss: 1.6263 - acc: 0.4191\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 9s 175us/step - loss: 1.1698 - acc: 0.5899\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 9s 174us/step - loss: 0.9993 - acc: 0.6522\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 9s 174us/step - loss: 0.8766 - acc: 0.6954\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 9s 175us/step - loss: 0.7719 - acc: 0.7306\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 9s 175us/step - loss: 0.6792 - acc: 0.7656\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 9s 174us/step - loss: 0.5991 - acc: 0.7915\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 9s 174us/step - loss: 0.5288 - acc: 0.8180\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 9s 175us/step - loss: 0.4561 - acc: 0.8412\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 9s 176us/step - loss: 0.4002 - acc: 0.8619\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 9s 176us/step - loss: 0.3569 - acc: 0.8777\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 9s 175us/step - loss: 0.3150 - acc: 0.8925\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 9s 175us/step - loss: 0.2908 - acc: 0.9007\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 9s 177us/step - loss: 0.2626 - acc: 0.9120\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 9s 175us/step - loss: 0.2449 - acc: 0.9185\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 9s 176us/step - loss: 0.2335 - acc: 0.9223\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 9s 175us/step - loss: 0.2131 - acc: 0.9308\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 9s 175us/step - loss: 0.2064 - acc: 0.9320\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 9s 175us/step - loss: 0.2007 - acc: 0.9352\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 9s 174us/step - loss: 0.1909 - acc: 0.9390\n",
            "10000/10000 [==============================] - 1s 127us/step\n",
            "\n",
            "Test loss: 1.207292466878891\n",
            "Test accuracy: 0.7361\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}